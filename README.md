# ML Pipeline with DVC and Remote Storage

A comprehensive machine learning pipeline project demonstrating best practices for data versioning, experiment tracking, and remote storage integration using DVC (Data Version Control).

## ğŸ“‹ Table of Contents

- [Project Overview](#project-overview)
- [Features](#features)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Usage](#usage)
- [DVC Pipeline Setup](#dvc-pipeline-setup)
- [Experiments](#experiments)
- [Contributing](#contributing)
- [License](#license)

## ğŸ¯ Project Overview

This project demonstrates a production-ready ML pipeline built with DVC for data versioning and experiment tracking. It includes components for data ingestion, preprocessing, and model training with comprehensive logging and parameter management.

## âœ¨ Features

- **Data Ingestion**: Load and split datasets with proper logging
- **Data Preprocessing**: Text transformation with NLTK (tokenization, stemming, stopword removal)
- **DVC Integration**: Version control for data, models, and pipeline stages
- **Experiment Tracking**: Track ML experiments with DVCLive
- **Parameter Management**: YAML-based configuration for reproducible pipelines
- **Comprehensive Logging**: File and console logging for all operations
- **Remote Storage Support**: Setup for cloud storage integration (S3, GCS, etc.)

## ğŸ“‚ Project Structure

```
ML_pipeline_with_DVC_with_remote_storage_practice/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_ingestion.py          # Data loading and train/test splitting
â”‚   â””â”€â”€ data_preprocessing.py       # Text preprocessing and transformation
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                        # Original raw data
â”‚   â”‚   â”œâ”€â”€ train.csv
â”‚   â”‚   â””â”€â”€ test.csv
â”‚   â”œâ”€â”€ interim/                    # Intermediate processed data
â”‚   â””â”€â”€ processed/                  # Final processed data
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ mynotebook.ipynb            # Jupyter notebook for experiments
â”‚   â””â”€â”€ spam.csv                    # Sample experiment data
â”œâ”€â”€ logs/                           # Application logs
â”‚   â”œâ”€â”€ data_ingestion.log
â”‚   â””â”€â”€ data_preprocessing.log
â”œâ”€â”€ dvc.yaml                        # DVC pipeline configuration
â”œâ”€â”€ params.yaml                     # Pipeline parameters
â”œâ”€â”€ .gitignore                      # Git ignore rules
â”œâ”€â”€ README.md                       # This file
â””â”€â”€ LICENSE                         # Project license
```

## ğŸš€ Installation

### Prerequisites

- Python 3.8 or higher
- Git
- DVC

### Setup Steps

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd ML_pipeline_with_DVC_with_remote_storage_practice
   ```

2. **Create a virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install pandas scikit-learn nltk pyyaml dvclive
   ```

4. **Initialize DVC**
   ```bash
   dvc init
   ```

5. **Configure remote storage** (optional)
   ```bash
   dvc remote add -d myremote s3://my-bucket/path
   # or for Google Cloud Storage
   dvc remote add -d myremote gs://my-bucket/path
   ```

## ğŸ“– Usage

### Run Data Ingestion

```bash
python src/data_ingestion.py
```

This script:
- Loads data from `data/raw/` directory
- Splits into train and test sets based on parameters
- Saves split data with logging to `logs/data_ingestion.log`

### Run Data Preprocessing

```bash
python src/data_preprocessing.py
```

This script:
- Reads raw training and test data
- Applies text preprocessing (lowercase, tokenization, stopword removal, stemming)
- Encodes target column
- Removes duplicates
- Saves processed data to `data/interim/` with logging to `logs/data_preprocessing.log`

## ğŸ”„ DVC Pipeline Setup

### 1. Initialize DVC Pipeline
```bash
dvc init
```

### 2. Create dvc.yaml
Define pipeline stages in `dvc.yaml`:
```yaml
stages:
  data_ingestion:
    cmd: python src/data_ingestion.py
    deps:
      - src/data_ingestion.py
      - data/raw/train.csv
      - data/raw/test.csv
    outs:
      - data/processed/train.csv
      - data/processed/test.csv

  data_preprocessing:
    cmd: python src/data_preprocessing.py
    deps:
      - src/data_preprocessing.py
      - data/raw/train.csv
    outs:
      - data/interim/train_processed.csv
```

### 3. Create params.yaml
Define parameters for reproducibility:
```yaml
data_ingestion:
  test_size: 0.2
  random_state: 42

preprocessing:
  text_column: "text"
  target_column: "target"
```

### 4. Run Pipeline
```bash
# Reproduce the entire pipeline
dvc repro

# Visualize the pipeline DAG
dvc dag
```

## ğŸ§ª Experiments

### Track Experiments with DVCLive

```bash
# Install DVCLive
pip install dvclive

# Run an experiment
dvc exp run

# View all experiments
dvc exp show

# Apply a previous experiment
dvc exp apply <exp-name>

# Remove an experiment
dvc exp remove <exp-name>
```

### Using VS Code Extension

1. Install the DVC extension in VS Code
2. Navigate to the DVC panel to:
   - View all experiments
   - Compare experiment results
   - Switch between experiments

## ğŸ“ Logs

Application logs are saved to the `logs/` directory:
- `data_ingestion.log`: Logs from data ingestion stage
- `data_preprocessing.log`: Logs from preprocessing stage

Log format: `TIMESTAMP - LOGGER_NAME - LOG_LEVEL - MESSAGE`

## ğŸ”§ Configuration

### Parameters File (params.yaml)

All pipeline parameters are managed through `params.yaml`. Modify this file to adjust:
- Train/test split ratio
- Random seeds for reproducibility
- Text preprocessing columns
- Other model hyperparameters

## ğŸ› Troubleshooting

### NLTK Resource Not Found
If you encounter `Resource punkt_tab not found`:
```python
import nltk
nltk.download('punkt_tab')
nltk.download('stopwords')
```

### DVC Remote Configuration
To setup remote storage:
```bash
# Add remote
dvc remote add -d myremote <remote-path>

# Push data to remote
dvc push

# Pull data from remote
dvc pull
```

## ğŸ“Š Best Practices

1. **Always use parameters.yaml** for configuration changes
2. **Commit parameters and .dvc files** to Git (not actual data)
3. **Run `dvc repro`** to ensure pipeline consistency
4. **Use `dvc exp run`** for experiment tracking
5. **Add meaningful commit messages** when pushing to Git/DVC

## ğŸ¤ Contributing

1. Create a feature branch: `git checkout -b feature/your-feature`
2. Make your changes and test thoroughly
3. Run the pipeline: `dvc repro`
4. Commit: `git commit -am 'Add feature'`
5. Push: `git push origin feature/your-feature`
6. Create a Pull Request

## ğŸ“„ License

This project is licensed under the LICENSE file - see the file for details.